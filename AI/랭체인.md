
- 비용이 걱정되면 LangServe와 Ollama를 사용하여 무료로 llm사용할 수 있다.
	- eeve 허깅페이스로부터 야놀자의 한국어모델

# 정의
- 거대언어모델 기반 앱을 매우 쉽고 빠르게 구축할 수 있도록 도와주는 프레임워크
- 챗봇에 메모리를 추가시키고, 프롬프트의 형태를 지정하고, 유효성을 검사하고, 출력을 파싱하고, 벡터를 저장하고 등등을 추상화된 함수로 제공
- 모델, 에이전트, 툴킷, 모니터링, 디버깅, 병렬화 등
- 파이프라인 기호인 | 가 마법처럼 쓰인다. 이 구문을 랭체인식 언어 또는 LCEL이라고 한다. (Langchain Expression Language). 체인끼리 연결할때 쓰인다.
- 프롬프트에 변수가 있는데 변수값이 다 할당안되면 api를 호출하기전에 유효성검사에서 걸러줘서 돈안쓰게 해준다.
- 응답을 나중에 챗봇 메모리에 넣거나 데이터베이스에 저장할 수 있는 AIMessage클래스를 제공한다.

### [강의 : 모두의 AI- 롱체인 부수기](https://www.youtube.com/watch?v=WWRCLzXxUgs&list=PLQIgLu3Wf-q_Ne8vv-ZXuJ4mztHJaQb_v)
- 랭체인 기본
	- [OpenAI](https://colab.research.google.com/drive/1Ain-2t_OI_llY0lSn0NEPJ1E7kNVdx5J?usp=sharing)
		- temperature = 0일수록 똑같은 답변, 1일수록 다양한 답변(신뢰도가 필요하면 0에 가깝도록)
		- 글자를 한번에 주지말고 타이핑치듯이 하는것 = from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
			- chatgpt = ChatOpenAI(model_name="gpt-3.5-turbo", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])
		- messages(3개)
			- SystemMessage = 봇의 역할을 지정
			- HumanMessage = 사람의 입력
			- AIMessage = 답변
			- // content = 그 각각에서 메타데이터 빼고 순수 문자열
	- [프롬프트](https://colab.research.google.com/drive/1wF8zTDDUdmq59RXYLOprje3yrpP8z9Pv?usp=sharing)
		- 모델에 대한 입력
		- 랭체인은 프롬프트를 쉽게 구성하고 작업할 수 있도록 여러 클래스와 함수를 제공
		 ```python
		    from langchain.prompts import PromptTemplate, ChatPromptTemplate
			template = """
			너는 요리사야. 내가 가진 재료들을 갖고 만들 수 있는 요리를 추천하고, 그 요리의 레시피를 제시해줘.
			내가 가진 재료는 아래와 같아.
			
			<재료>
			{재료}
			
			"""
			prompt_template = PromptTemplate(
			    input_variables = ['재료'],
			    template = template
			)
		   ```
		- chatGpt
		  ```python
		from langchain.prompts import (
		    ChatPromptTemplate,
		    PromptTemplate,
		    SystemMessagePromptTemplate,
		    AIMessagePromptTemplate,
		    HumanMessagePromptTemplate,
		)
		from langchain.schema import (
		    AIMessage,
		    HumanMessage,
		    SystemMessage
		
		# ChatGPT 모델을 로드합니다.
		chatgpt = ChatOpenAI(temperature=0)
		
		#ChatGPT에게 역할을 부여합니다.(위에서 정의한 Template 사용)
		system_message_prompt = SystemMessagePromptTemplate.from_template(template)
		
		#사용자가 입력할 매개변수 template을 선언합니다.
		human_template = "{재료}"
		human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
		
		#ChatPromptTemplate에 system message와 human message 템플릿을 삽입합니다.
		chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])
		
		#ChatGPT API에 ChatPromptTemplate을 입력할 때, human message의 매개변수인 '재료'를 할당하여 전달합니다.
		#이와 같은 방식을 통해 ChatGPT는 ChatPromptTemplate의 구성요소인 system message와 human message를 전달받아, 대답 생성에 활용합니다.
		answer = chatgpt(chat_prompt.format_prompt(재료="양파, 계란, 사과, 빵").to_messages())
			```
			- Few-shot
				- 결과물의 형태를 구조화하여 예제를 주면 그 형식대로 답변을 하게됨
				- 삼행시형태, 반대단어 출력 등
				- SemanticSimilarityExampleSelector(사용자가입력한것과 비슷한예시), Chroma(벡터로 비교), OpenAIEmbeddings(문자를 숫자로 수치화)
	- [Retriever](https://colab.research.google.com/drive/1S3jKF6Jofvl48fHUFqQwS-0iWMgl-pWl?usp=sharing)
		- 공식설명 : 비정형 쿼리가 주어지면 문서를 반환하는 인터페이스.  벡터 저장소보다 더 일반적.  문서를 저장할 필요없이 단지 반환(또는 검색)만 할 수 있다.
		- 사용자의 질문을 임베딩 모델을 통해 벡터화하여 벡터저장소에 저장하고 그 안에 있는 여러 청크벡터들과 비교하여 가장 유사한 벡터를 뽑아서 컨텍스트로 넘기고  사용자의 질문과 함께 프롬프트로 엮어서 llm에게 넘겨주는 이 모든 역할에 리트리버가 관여를 한다.
		- 분할된 텍스트 청크를 Context에 그대로 주입한다.
			- 어떤식으로 참고할 자료를 넘겨주는지에 따라 4가지의 체인이 존재한다.
			- Stuff (질문과, 비슷한 질문을 같이 전달.       한계 : 참고하고 주는 컨텍스트 텍스트 청크가 너무 길면 토큰초과)
			- Map Reduce(스터프와 같지만 참고하는 텍스트청크에 대해 모두 요약후 최종요약.   한계 : 다수 호출이 필요하여 속도가 느리고 비용이슈)
			- Refine(누적.     한계 : 품질이 뛰어나지만 시간이 오래걸림)
			- Map ReRank(분할된 텍스트 청크를 순회하면서 누적 답변 생성. answer에 score도 받아서 가장 높은점수인 1개를 반환.    한계 : 품질이 뛰어나지만 시간이 오래걸림)



## 청크사이즈
- 한번에 처리할수 있는 텍스트의 최대길이
- 모델이 최대 토큰수에 가깝게 처리할 경우 메모리 사용량이 많아지고 때로는 오류가 발생하니까 최대토큰수의 약 80~90% 정도를 청크사이즈로 설정하는것이 좋다.